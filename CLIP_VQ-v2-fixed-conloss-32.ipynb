{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411cefa4-6b15-4e1e-847b-4349cce08da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n",
      "11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/envs/test/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os, sys\n",
    "import requests\n",
    "import PIL\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from dall_e          import map_pixels, unmap_pixels, load_model\n",
    "from IPython.display import display, display_markdown\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e298ee3-9f64-455f-9da9-edf3b9aeebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import skimage\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694c2363-d1dd-4dcb-94e2-2b288e14b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "298dc7da-73a5-4443-ad70-7bead6cdf497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140002d4-9793-4030-b542-5c345fbad812",
   "metadata": {},
   "source": [
    "## visual codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4be599a-5c9e-4e95-81e0-24b2ee2f78ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5978ad59-bcdf-488b-bc2b-d6af2a8bdd5d",
   "metadata": {},
   "source": [
    "## clip image processing and text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f4b003c-c7eb-4016-a002-d817a39e3be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "310e282b-5b9e-47c0-aeff-5c47d2bb1e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"/hy-tmp/clip_model/ViT-B-32.pt\", device)\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5a6ca6f-7f9d-4a11-b556-ffda5fe373c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7f6dbb4829d0>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "358980b6-902b-4166-97ed-4f2b932cf354",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_knowledge = [\"The number of islands on an aircraft carrier is 1, the bow shape is blunt, and the hull has a flat runway.\", \n",
    "\"The number of islands on a destroyer is 1, the bow shape is pointed, and the hull has a fluctuating island.\", \n",
    "\"The number of islands on a cruiser is 2, the bow shape is pointed, and the hull has a fluctuating island.\", \n",
    "\"The number of islands on a supply ship is 2, the bow shape is pointed, and the hull has a fluctuating gantry and a fluctuating island.\", \n",
    "\"A cruise ship has no islands, the bow shape is pointed, and the hull has cabins.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0571e58b-a6db-493f-bab4-5e28e447e8a4",
   "metadata": {},
   "source": [
    "注释：有字符的地方会编码为向量，没有字符的地方就是0，维度固定为77是因为给句子规定了一个最大长度77。另外注意：句首和句末分别有一个起始符和一个终止符。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4900d6fb-f408-4e10-bc6e-2d73b614d557",
   "metadata": {},
   "source": [
    "CoOp原文中使用的token数量是16。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0361f-5a55-4293-9e20-ac60292bb122",
   "metadata": {},
   "source": [
    "## self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56728b8a-73db-4e3d-9a20-1ef09e4597c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KnowledgeTransformer(vit, x, contexts):\n",
    "    x = vit.conv1(x)  # shape = [*, width, grid, grid]\n",
    "    x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "    x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "    x = torch.cat([vit.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "    x = x + vit.positional_embedding.to(x.dtype)\n",
    "    x = vit.ln_pre(x)\n",
    "    # print(x.size())\n",
    "    # print(contexts.size())\n",
    "    \n",
    "    contexts = contexts.repeat(x.size()[0], 1, 1).to(device)\n",
    "    # print(contexts.size())\n",
    "    x = torch.cat((x, contexts), 1).half()\n",
    "    # print(x.size())\n",
    "    x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "    x = vit.transformer(x)\n",
    "    x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "    x = vit.ln_post(x[:, 0, :])\n",
    "\n",
    "    if vit.proj is not None:\n",
    "        x = x @ vit.proj\n",
    "    return x\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_attention_heads, input_size_q, input_size_kv, hidden_size, hidden_dropout_prob):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        if hidden_size % num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "        self.all_head_size = hidden_size\n",
    "\n",
    "        self.query = nn.Linear(input_size_q, self.all_head_size)\n",
    "        self.key = nn.Linear(input_size_kv, self.all_head_size)\n",
    "        self.value = nn.Linear(input_size_kv, self.all_head_size)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "        # 做完self-attention 做一个前馈全连接 LayerNorm 输出\n",
    "        # self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.LayerNorm = LayerNorm(hidden_size, eps=1e-12)\n",
    "        # self.out_dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        # print(new_x_shape)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, input_q, input_k, input_v):\n",
    "        query_layer = self.transpose_for_scores(self.query(input_q))\n",
    "        key_layer = self.transpose_for_scores(self.key(input_k))\n",
    "        value_layer = self.transpose_for_scores(self.value(input_v))\n",
    "        \n",
    "        # Cross-attention\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context = context_layer.view(*new_context_layer_shape)\n",
    "        # hidden_states = self.dense(context_layer)\n",
    "        # hidden_states = self.out_dropout(hidden_states)\n",
    "        # hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return context\n",
    "\n",
    "    \n",
    "class LinearClassifier(nn.Module): \n",
    "    def __init__(self, input_dim, output_dim): \n",
    "        super(LinearClassifier, self).__init__() \n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = self.fc(x)\n",
    "        # print(x.size())\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "        \n",
    "class ContrastiveClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContrastiveClassifier, self).__init__()\n",
    "        self.t = 0.07\n",
    "        \n",
    "    def forward(self, image_feature, text_feature):\n",
    "        # image_feature = torch.norm(image_feature, dim=-1)  # 32, 768\n",
    "        # text_feature = torch.norm(text_feature, dim=-1).squeeze(0)    #[5,768]->\n",
    "        # logits = (image_feature @ text_feature.T) * torch.exp(self.t) #32，5\n",
    "        \n",
    "        # 这样写似乎容易出问题\n",
    "        # image_feature /= image_feature.norm(dim=-1, keepdim=True)\n",
    "        # text_feature /= text_feature.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        image_feature = F.normalize(image_feature, p=2, dim=-1)\n",
    "        text_feature = F.normalize(text_feature, p=2, dim=-1)\n",
    "\n",
    "        logits = (100.0 * image_feature @ text_feature.T).softmax(dim=-1)\n",
    "        return logits\n",
    "        \n",
    "        \n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, n_context):\n",
    "        super().__init__()\n",
    "        self.transformer = model.transformer\n",
    "        self.positional_embedding = model.positional_embedding\n",
    "        self.ln_final = model.ln_final\n",
    "        self.text_projection = model.text_projection\n",
    "        self.dtype = model.dtype\n",
    "        self.n_context = n_context\n",
    "\n",
    "    def forward(self, prompts, tokenized_prompts, context_feature):\n",
    "        x = prompts + self.positional_embedding\n",
    "        \n",
    "        x = x.permute(1, 0, 2).to(torch.float16)  # NLD -> LND\n",
    "        # print(x.dtype)\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.ln_final(x).type(self.dtype)\n",
    "        \n",
    "        if context_feature:\n",
    "            x = x[:, 1: self.n_context + 1, :]  # x.shape is [5, 16, 512]\n",
    "            tokenized_prompts = tokenized_prompts[:, 1: self.n_context]\n",
    "        \n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
    "\n",
    "        return x\n",
    "        \n",
    "class PromptLearner(nn.Module):\n",
    "    def __init__(self, expert_knowledge):\n",
    "        super().__init__()\n",
    "        n_context = 32  # 与coop一致\n",
    "        context_dim = model.ln_final.weight.shape[0]  # 512\n",
    "        len_knowledge = len(expert_knowledge)\n",
    "        \n",
    "        context_vectors = torch.empty(1, n_context, context_dim)\n",
    "        context_vectors = context_vectors.repeat(len_knowledge, 1, 1).to(device)\n",
    "        nn.init.normal_(context_vectors, std=0.02)\n",
    "        \n",
    "        self.context = nn.Parameter(context_vectors)\n",
    "        \n",
    "        prompt_prefix = \" \".join([\"X\"] * n_context)\n",
    "        prompts = [prompt_prefix + \" \" + kl for kl in expert_knowledge]\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(device)\n",
    "        with torch.no_grad():\n",
    "            embedding = model.token_embedding(tokenized_prompts)\n",
    "        \n",
    "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # Start Of the Sentence\n",
    "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_context :, :])  # Expert Knowledge, End of Sentence\n",
    "        \n",
    "        self.n_context = n_context\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "        \n",
    "    def forward(self):\n",
    "        context = self.context\n",
    "        \n",
    "        prefix = self.token_prefix\n",
    "        suffix = self.token_suffix\n",
    "        prompts = torch.cat([prefix, context, suffix], dim=1)\n",
    "        \n",
    "        return prompts, context\n",
    "        \n",
    "\n",
    "class MyKnowledgeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyKnowledgeNet, self).__init__()\n",
    "        \n",
    "        enc = load_model(\"/hy-tmp/vae/encoder.pkl\", device)\n",
    "        dec = load_model(\"/hy-tmp/vae/decoder.pkl\", device)\n",
    "        params = enc.state_dict()  # 提取出的visual codebook的参数\n",
    "        self.vc_weight = params[\"blocks.output.conv.w\"]\n",
    "        self.vc_weight = self.vc_weight.squeeze(2).squeeze(2).unsqueeze(0)\n",
    "        \n",
    "        num_attention_heads = 8\n",
    "        input_size_q = 512\n",
    "        input_size_kv = 2048\n",
    "        hidden_size = 768\n",
    "        hidden_dropout_prob = 0.1\n",
    "        self.self_attention = SelfAttention(num_attention_heads, input_size_q, input_size_kv, hidden_size, hidden_dropout_prob).to(device)\n",
    "        \n",
    "        input_dim = 512\n",
    "        output_dim = 5\n",
    "        self.linear = LinearClassifier(input_dim, output_dim).to(device)\n",
    "        \n",
    "        self.contrastive = ContrastiveClassifier().to(device)\n",
    "        \n",
    "        self.prompt_learner = PromptLearner(expert_knowledge).to(device)\n",
    "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
    "        self.n_context = self.prompt_learner.n_context\n",
    "        \n",
    "        self.text_encoder = TextEncoder(self.n_context)\n",
    "        \n",
    "        self.vt = model.visual.to(device)\n",
    "        \n",
    "        self.embedding_projection = nn.Linear(512, 768)  # 512是文本向量的维度，768是图像向量的维度，学习这样一个投影层\n",
    "\n",
    "    def forward(self, images):\n",
    "        \n",
    "        prompts, context = self.prompt_learner()  # context.size()=[5,16,512]\n",
    "        text_feature = self.text_encoder(prompts, self.tokenized_prompts, context_feature=False)\n",
    "        text_feature = text_feature.to(torch.float32)\n",
    "        \n",
    "        context = context.mean(dim=0, keepdim=True).to(torch.float32)\n",
    "        context = context[:, :16, :]\n",
    "        k = self.self_attention(context, self.vc_weight, self.vc_weight)\n",
    "        \n",
    "        image_feature = KnowledgeTransformer(self.vt, images, k).float()\n",
    "        CLS_vector = self.contrastive(image_feature, text_feature)\n",
    "        return CLS_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5a418-9493-46a1-925f-f3fa7b9e95ac",
   "metadata": {},
   "source": [
    "一个问题：如何让context的维度能够使用？是否应该重视每个单词的意义？我想肯定要重视。mean的方法可靠性存疑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2ea8dc3-7087-4a38-9f10-d82887d94ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir):\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        label_dict = {'Aircraft_Carrier': 0, \n",
    "                      'Amphibious_Assault_Ship': 1, \n",
    "                      'Fast_Combat_Support_Ships': 2,\n",
    "                      'Guided_Missile_Cruiser': 3,\n",
    "                      'Guided_Missile_Destroyer': 4}\n",
    "        for label in os.listdir(img_dir):\n",
    "            dir_path = os.path.join(img_dir, label)\n",
    "            for img in os.listdir(dir_path):\n",
    "                img_path = os.path.join(dir_path, img)\n",
    "                img_label = label\n",
    "                self.imgs.append(img_path)\n",
    "                self.labels.append(label_dict[img_label])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs[index]\n",
    "        label = self.labels[index]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d459bf8-1d5c-4acc-bd32-6494c5e246c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ships = ImageDataset(\"/hy-tmp/5_types_ships_small/train\")\n",
    "test_ships = ImageDataset(\"/hy-tmp/5_types_ships_small/test\")\n",
    "train_loader = torch.utils.data.DataLoader(img_ships, batch_size=96, shuffle=True, drop_last=False, num_workers=32)\n",
    "test_loader = torch.utils.data.DataLoader(test_ships, batch_size=96, shuffle=False, drop_last=False, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f7d8230-be62-46dd-a351-95e047df95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = MyKnowledgeNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a76f79b-e447-4e3a-817e-638f72a75bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# backbone_params = list(map(id, mymodel.vt.parameters()))\n",
    "# align_parmas = filter(lambda p: id(p) not in backbone_params, mymodel.parameters())\n",
    "\n",
    "optimizer = optim.Adam([{'params': mymodel.self_attention.parameters()},\n",
    "                       # {'params': mymodel.linear.parameters()}, \n",
    "                       # {'params': mymodel.contrastive.parameters()}, \n",
    "                       {'params': mymodel.embedding_projection.parameters()},\n",
    "                       {'params': mymodel.prompt_learner.parameters()}], lr=1e-3)\n",
    "# optimizer = optimizer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ca9cc47-361b-4e1b-b6e0-73f7dc1b296b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/4052 (0%)]\tLoss: 1.631668\n",
      "Train Epoch: 1 [0/4052 (0%)]\tLoss: 1.188619\n",
      "Train Epoch: 2 [0/4052 (0%)]\tLoss: 1.232353\n",
      "Train Epoch: 3 [0/4052 (0%)]\tLoss: 1.213001\n",
      "Train Epoch: 4 [0/4052 (0%)]\tLoss: 1.146899\n",
      "Train Epoch: 5 [0/4052 (0%)]\tLoss: 1.116716\n",
      "Train Epoch: 6 [0/4052 (0%)]\tLoss: 1.235353\n",
      "Train Epoch: 7 [0/4052 (0%)]\tLoss: 1.120965\n",
      "Train Epoch: 8 [0/4052 (0%)]\tLoss: 1.111047\n",
      "Train Epoch: 9 [0/4052 (0%)]\tLoss: 1.150627\n",
      "Train Epoch: 10 [0/4052 (0%)]\tLoss: 1.143454\n",
      "Train Epoch: 11 [0/4052 (0%)]\tLoss: 1.105760\n",
      "Train Epoch: 12 [0/4052 (0%)]\tLoss: 1.051099\n",
      "Train Epoch: 13 [0/4052 (0%)]\tLoss: 1.108668\n",
      "Train Epoch: 14 [0/4052 (0%)]\tLoss: 1.045103\n",
      "Train Epoch: 15 [0/4052 (0%)]\tLoss: 1.131175\n",
      "Train Epoch: 16 [0/4052 (0%)]\tLoss: 1.123112\n",
      "Train Epoch: 17 [0/4052 (0%)]\tLoss: 1.063764\n",
      "Train Epoch: 18 [0/4052 (0%)]\tLoss: 1.015022\n",
      "Train Epoch: 19 [0/4052 (0%)]\tLoss: 0.996504\n",
      "Train Epoch: 20 [0/4052 (0%)]\tLoss: 1.075999\n",
      "Train Epoch: 21 [0/4052 (0%)]\tLoss: 1.005803\n",
      "Train Epoch: 22 [0/4052 (0%)]\tLoss: 1.085454\n",
      "Train Epoch: 23 [0/4052 (0%)]\tLoss: 1.027082\n",
      "Train Epoch: 24 [0/4052 (0%)]\tLoss: 1.051671\n",
      "Train Epoch: 25 [0/4052 (0%)]\tLoss: 1.095114\n",
      "Train Epoch: 26 [0/4052 (0%)]\tLoss: 1.052871\n",
      "Train Epoch: 27 [0/4052 (0%)]\tLoss: 1.044618\n",
      "Train Epoch: 28 [0/4052 (0%)]\tLoss: 1.056506\n",
      "Train Epoch: 29 [0/4052 (0%)]\tLoss: 1.063459\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "for epoch in range(30):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        labels = labels.to(device)\n",
    "        images = [preprocess(PIL.Image.open(image)) for image in inputs]\n",
    "        image_input = torch.tensor(np.stack(images)).half().to(device)\n",
    "\n",
    "#         output = mymodel(image_input)\n",
    "#         loss = criterion(output, labels)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward(retain_graph=True)\n",
    "#         optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()  # move zero_grad before the forward pass\n",
    "        output = mymodel(image_input)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, i * len(data), len(train_loader.dataset), 100. * i / len(train_loader), loss.item()))\n",
    "            \n",
    "            \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eabc4210-73ac-4954-bccf-5a23784ba1c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vt = model.visual\n",
    "# y = KnowledgeTransformer(vt, image_input, x)\n",
    "# print(y)\n",
    "# print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32846677-0a72-46c4-9534-25dcaa386622",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mymodel.state_dict(), '/hy-tmp/model/model_state_dict_v2_fixed_conloss_32.ptl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "357b363a-2690-448e-8dc8-8967e0cd4e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mymodel, '/hy-tmp/model/test_model_v2_fixed_conloss_32.ptl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdade962-40d8-4f13-8d2f-759022aa1862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.load_state_dict(torch.load('/hy-tmp/model/model_state_dict_v2_fixed_conloss_32.ptl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2134ccf4-1a76-4fb6-93c6-8a1d49d7ff82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy of the network on the 449 test images: 77.28 %\n",
      "Top-2 accuracy of the network on the 449 test images: 88.64 %\n",
      "Top-3 accuracy of the network on the 449 test images: 94.65 %\n"
     ]
    }
   ],
   "source": [
    "top_1_correct = 0\n",
    "top_2_correct = 0\n",
    "top_3_correct = 0\n",
    "total = 0\n",
    "# set the model to evaluation mode\n",
    "mymodel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        labels = labels.to(device)\n",
    "        images = [preprocess(PIL.Image.open(image)) for image in inputs]\n",
    "        image_input = torch.tensor(np.stack(images)).half().to(device)\n",
    "        \n",
    "        outputs = mymodel(image_input)\n",
    "        _, predicted = torch.topk(outputs.data, k=3, dim=1)\n",
    "        total += labels.size(0)\n",
    "        top_1_correct += (predicted[:, 0] == labels).sum().item()\n",
    "        top_2_correct += ((predicted[:, 0] == labels) | (predicted[:, 1] == labels)).sum().item()\n",
    "        top_3_correct += ((predicted[:, 0] == labels) | (predicted[:, 1] == labels) | (predicted[:, 2] == labels)).sum().item()\n",
    "\n",
    "print('Top-1 accuracy of the network on the %d test images: %.2f %%' % (len(test_loader.dataset), 100 * top_1_correct / total))\n",
    "print('Top-2 accuracy of the network on the %d test images: %.2f %%' % (len(test_loader.dataset), 100 * top_2_correct / total))\n",
    "print('Top-3 accuracy of the network on the %d test images: %.2f %%' % (len(test_loader.dataset), 100 * top_3_correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd64894-9817-499a-bccf-1490efdc8229",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
