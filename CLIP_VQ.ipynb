{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411cefa4-6b15-4e1e-847b-4349cce08da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n",
      "11.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/miniconda3/envs/test/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import os, sys\n",
    "import requests\n",
    "import PIL\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from dall_e          import map_pixels, unmap_pixels, load_model\n",
    "from IPython.display import display, display_markdown\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e298ee3-9f64-455f-9da9-edf3b9aeebd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import skimage\n",
    "\n",
    "from collections import OrderedDict\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694c2363-d1dd-4dcb-94e2-2b288e14b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140002d4-9793-4030-b542-5c345fbad812",
   "metadata": {},
   "source": [
    "## visual codebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4be599a-5c9e-4e95-81e0-24b2ee2f78ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5978ad59-bcdf-488b-bc2b-d6af2a8bdd5d",
   "metadata": {},
   "source": [
    "## clip image processing and text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f4b003c-c7eb-4016-a002-d817a39e3be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "310e282b-5b9e-47c0-aeff-5c47d2bb1e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"/hy-tmp/clip_model/ViT-B-32.pt\", device)\n",
    "model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5a6ca6f-7f9d-4a11-b556-ffda5fe373c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x7f9253fe9430>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "358980b6-902b-4166-97ed-4f2b932cf354",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_knowledge = [\"The number of islands on an aircraft carrier is 1, the bow shape is blunt, and the hull has a flat runway.\", \n",
    "\"The number of islands on a destroyer is 1, the bow shape is pointed, and the hull has a fluctuating island.\", \n",
    "\"The number of islands on a cruiser is 2, the bow shape is pointed, and the hull has a fluctuating island.\", \n",
    "\"The number of islands on a supply ship is 2, the bow shape is pointed, and the hull has a fluctuating gantry and a fluctuating island.\", \n",
    "\"A cruise ship has no islands, the bow shape is pointed, and the hull has cabins.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ac68a81-d5b9-45f7-b040-b7a9c9eb9fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tensor = clip.tokenize(expert_knowledge).to(device)\n",
    "ek_tensor = model.encode_text(text_tensor).float()\n",
    "ek_tensor = torch.unsqueeze(ek_tensor, 2)\n",
    "ek_tensor = torch.unsqueeze(ek_tensor, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cc71fc8-391b-4d86-b82b-f5e137b1a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "ek_tensor = ek_tensor.squeeze(2).squeeze(2).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0361f-5a55-4293-9e20-ac60292bb122",
   "metadata": {},
   "source": [
    "## self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56728b8a-73db-4e3d-9a20-1ef09e4597c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KnowledgeTransformer(vit, x, contexts):\n",
    "    x = vit.conv1(x)  # shape = [*, width, grid, grid]\n",
    "    x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]\n",
    "    x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]\n",
    "    x = torch.cat([vit.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]\n",
    "    x = x + vit.positional_embedding.to(x.dtype)\n",
    "    x = vit.ln_pre(x)\n",
    "    # print(x.size())\n",
    "    \n",
    "    contexts = contexts.repeat(x.size()[0], 1, 1).to(device)\n",
    "    x = torch.cat((x, contexts), 1).half()\n",
    "    # print(x.size())\n",
    "    x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "    x = vit.transformer(x)\n",
    "    x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "\n",
    "    x = vit.ln_post(x[:, 0, :])\n",
    "\n",
    "    if vit.proj is not None:\n",
    "        x = x @ vit.proj\n",
    "    return x\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, num_attention_heads, input_size_q, input_size_kv, hidden_size, hidden_dropout_prob):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        if hidden_size % num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (hidden_size, num_attention_heads))\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.attention_head_size = int(hidden_size / num_attention_heads)\n",
    "        self.all_head_size = hidden_size\n",
    "\n",
    "        self.query = nn.Linear(input_size_q, self.all_head_size)\n",
    "        self.key = nn.Linear(input_size_kv, self.all_head_size)\n",
    "        self.value = nn.Linear(input_size_kv, self.all_head_size)\n",
    "\n",
    "        self.attn_dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "        # 做完self-attention 做一个前馈全连接 LayerNorm 输出\n",
    "        # self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "        # self.LayerNorm = LayerNorm(hidden_size, eps=1e-12)\n",
    "        # self.out_dropout = nn.Dropout(hidden_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        # print(new_x_shape)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, input_q, input_k, input_v):\n",
    "        query_layer = self.transpose_for_scores(self.query(input_q))\n",
    "        key_layer = self.transpose_for_scores(self.key(input_k))\n",
    "        value_layer = self.transpose_for_scores(self.value(input_v))\n",
    "        \n",
    "        # Cross-attention\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        \n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context = context_layer.view(*new_context_layer_shape)\n",
    "        # hidden_states = self.dense(context_layer)\n",
    "        # hidden_states = self.out_dropout(hidden_states)\n",
    "        # hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return context\n",
    "\n",
    "    \n",
    "class LinearClassifier(nn.Module): \n",
    "    def __init__(self, input_dim, output_dim): \n",
    "        super(LinearClassifier, self).__init__() \n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = self.fc(x)\n",
    "        # print(x.size())\n",
    "        return F.log_softmax(x, dim=1)\n",
    "        \n",
    "        \n",
    "class MyKnowledgeNet(nn.Module):\n",
    "    def __init__(self, ek_tensor):\n",
    "        super(MyKnowledgeNet, self).__init__()\n",
    "        \n",
    "        enc = load_model(\"/hy-tmp/vae/encoder.pkl\", device)\n",
    "        dec = load_model(\"/hy-tmp/vae/decoder.pkl\", device)\n",
    "        params = enc.state_dict()  # 提取出的visual codebook的参数\n",
    "        self.vc_weight = params[\"blocks.output.conv.w\"]\n",
    "        self.vc_weight = self.vc_weight.squeeze(2).squeeze(2).unsqueeze(0)\n",
    "        self.ek_tensor = ek_tensor\n",
    "        \n",
    "        \n",
    "        num_attention_heads = 8\n",
    "        input_size_q = 512\n",
    "        input_size_kv = 2048\n",
    "        hidden_size = 768\n",
    "        hidden_dropout_prob = 0.1\n",
    "        self.self_attention = SelfAttention(num_attention_heads, input_size_q, input_size_kv, hidden_size, hidden_dropout_prob).to(device)\n",
    "        \n",
    "        input_dim = 512\n",
    "        output_dim = 5\n",
    "        self.linear = LinearClassifier(input_dim, output_dim).to(device)\n",
    "        \n",
    "        self.vt = model.visual.to(device)\n",
    "\n",
    "    def forward(self, i):\n",
    "        k = self.self_attention(self.ek_tensor, self.vc_weight, self.vc_weight)\n",
    "        i = KnowledgeTransformer(self.vt, i, k).float()\n",
    "        i = self.linear(i)\n",
    "        return i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2ea8dc3-7087-4a38-9f10-d82887d94ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, img_dir):\n",
    "        self.imgs = []\n",
    "        self.labels = []\n",
    "        label_dict = {'Aircraft_Carrier': 0, \n",
    "                      'Amphibious_Assault_Ship': 1, \n",
    "                      'Fast_Combat_Support_Ships': 2,\n",
    "                      'Guided_Missile_Cruiser': 3,\n",
    "                      'Guided_Missile_Destroyer': 4}\n",
    "        for label in os.listdir(img_dir):\n",
    "            dir_path = os.path.join(img_dir, label)\n",
    "            for img in os.listdir(dir_path):\n",
    "                img_path = os.path.join(dir_path, img)\n",
    "                img_label = label\n",
    "                self.imgs.append(img_path)\n",
    "                self.labels.append(label_dict[img_label])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = self.imgs[index]\n",
    "        label = self.labels[index]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d459bf8-1d5c-4acc-bd32-6494c5e246c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ships = ImageDataset(\"/hy-tmp/5_types_ships_small/train\")\n",
    "test_ships = ImageDataset(\"/hy-tmp/5_types_ships_small/test\")\n",
    "train_loader = torch.utils.data.DataLoader(img_ships, batch_size=96, shuffle=True, drop_last=False, num_workers=32)\n",
    "test_loader = torch.utils.data.DataLoader(test_ships, batch_size=96, shuffle=False, drop_last=False, num_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f7d8230-be62-46dd-a351-95e047df95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = MyKnowledgeNet(ek_tensor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a76f79b-e447-4e3a-817e-638f72a75bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# backbone_params = list(map(id, mymodel.vt.parameters()))\n",
    "# align_parmas = filter(lambda p: id(p) not in backbone_params, mymodel.parameters())\n",
    "\n",
    "optimizer = optim.Adam([{'params': mymodel.self_attention.parameters()},\n",
    "                       {'params': mymodel.linear.parameters()}], lr=1e-3)\n",
    "# optimizer = optimizer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ca9cc47-361b-4e1b-b6e0-73f7dc1b296b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/4052 (0%)]\tLoss: 1.601535\n",
      "Train Epoch: 1 [0/4052 (0%)]\tLoss: 1.131771\n",
      "Train Epoch: 2 [0/4052 (0%)]\tLoss: 0.868758\n",
      "Train Epoch: 3 [0/4052 (0%)]\tLoss: 0.950827\n",
      "Train Epoch: 4 [0/4052 (0%)]\tLoss: 0.764624\n",
      "Train Epoch: 5 [0/4052 (0%)]\tLoss: 0.617154\n",
      "Train Epoch: 6 [0/4052 (0%)]\tLoss: 0.624229\n",
      "Train Epoch: 7 [0/4052 (0%)]\tLoss: 0.752149\n",
      "Train Epoch: 8 [0/4052 (0%)]\tLoss: 0.515592\n",
      "Train Epoch: 9 [0/4052 (0%)]\tLoss: 0.669451\n",
      "Train Epoch: 10 [0/4052 (0%)]\tLoss: 0.710142\n",
      "Train Epoch: 11 [0/4052 (0%)]\tLoss: 0.569979\n",
      "Train Epoch: 12 [0/4052 (0%)]\tLoss: 0.607095\n",
      "Train Epoch: 13 [0/4052 (0%)]\tLoss: 0.514314\n",
      "Train Epoch: 14 [0/4052 (0%)]\tLoss: 0.537376\n",
      "Train Epoch: 15 [0/4052 (0%)]\tLoss: 0.510668\n",
      "Train Epoch: 16 [0/4052 (0%)]\tLoss: 0.560017\n",
      "Train Epoch: 17 [0/4052 (0%)]\tLoss: 0.463851\n",
      "Train Epoch: 18 [0/4052 (0%)]\tLoss: 0.600072\n",
      "Train Epoch: 19 [0/4052 (0%)]\tLoss: 0.429660\n",
      "Train Epoch: 20 [0/4052 (0%)]\tLoss: 0.467833\n",
      "Train Epoch: 21 [0/4052 (0%)]\tLoss: 0.422141\n",
      "Train Epoch: 22 [0/4052 (0%)]\tLoss: 0.556762\n",
      "Train Epoch: 23 [0/4052 (0%)]\tLoss: 0.507227\n",
      "Train Epoch: 24 [0/4052 (0%)]\tLoss: 0.473460\n",
      "Train Epoch: 25 [0/4052 (0%)]\tLoss: 0.498535\n",
      "Train Epoch: 26 [0/4052 (0%)]\tLoss: 0.435637\n",
      "Train Epoch: 27 [0/4052 (0%)]\tLoss: 0.516098\n",
      "Train Epoch: 28 [0/4052 (0%)]\tLoss: 0.427802\n",
      "Train Epoch: 29 [0/4052 (0%)]\tLoss: 0.413434\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "for epoch in range(30):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        labels = labels.to(device)\n",
    "        images = [preprocess(PIL.Image.open(image)) for image in inputs]\n",
    "        image_input = torch.tensor(np.stack(images)).half().to(device)\n",
    "\n",
    "#         output = mymodel(image_input)\n",
    "#         loss = criterion(output, labels)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward(retain_graph=True)\n",
    "#         optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()  # move zero_grad before the forward pass\n",
    "        output = mymodel(image_input)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, i * len(data), len(train_loader.dataset), 100. * i / len(train_loader), loss.item()))\n",
    "            \n",
    "            \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eabc4210-73ac-4954-bccf-5a23784ba1c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vt = model.visual\n",
    "# y = KnowledgeTransformer(vt, image_input, x)\n",
    "# print(y)\n",
    "# print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32846677-0a72-46c4-9534-25dcaa386622",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mymodel.state_dict(), '/hy-tmp/model/model_state_dict.ptl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "357b363a-2690-448e-8dc8-8967e0cd4e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(mymodel, '/hy-tmp/model/test_model.ptl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdade962-40d8-4f13-8d2f-759022aa1862",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.load_state_dict(torch.load('/hy-tmp/model/model_state_dict.ptl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2134ccf4-1a76-4fb6-93c6-8a1d49d7ff82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 accuracy of the network on the 449 test images: 78.40 %\n",
      "Top-2 accuracy of the network on the 449 test images: 91.76 %\n",
      "Top-3 accuracy of the network on the 449 test images: 95.55 %\n"
     ]
    }
   ],
   "source": [
    "top_1_correct = 0\n",
    "top_2_correct = 0\n",
    "top_3_correct = 0\n",
    "total = 0\n",
    "# set the model to evaluation mode\n",
    "mymodel.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        labels = labels.to(device)\n",
    "        images = [preprocess(PIL.Image.open(image)) for image in inputs]\n",
    "        image_input = torch.tensor(np.stack(images)).half().to(device)\n",
    "        \n",
    "        outputs = mymodel(image_input)\n",
    "        _, predicted = torch.topk(outputs.data, k=3, dim=1)\n",
    "        total += labels.size(0)\n",
    "        top_1_correct += (predicted[:, 0] == labels).sum().item()\n",
    "        top_2_correct += ((predicted[:, 0] == labels) | (predicted[:, 1] == labels)).sum().item()\n",
    "        top_3_correct += ((predicted[:, 0] == labels) | (predicted[:, 1] == labels) | (predicted[:, 2] == labels)).sum().item()\n",
    "\n",
    "print('Top-1 accuracy of the network on the %d test images: %.2f %%' % (len(test_loader.dataset), 100 * top_1_correct / total))\n",
    "print('Top-2 accuracy of the network on the %d test images: %.2f %%' % (len(test_loader.dataset), 100 * top_2_correct / total))\n",
    "print('Top-3 accuracy of the network on the %d test images: %.2f %%' % (len(test_loader.dataset), 100 * top_3_correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
