{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5195c2f-648e-480c-aa2b-7ca97882597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# pylint: disable=arguments-differ\n",
    "\n",
    "\n",
    "def initialize_weight(x):\n",
    "    nn.init.xavier_uniform_(x.weight)\n",
    "    if x.bias is not None:\n",
    "        nn.init.constant_(x.bias, 0)\n",
    "\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(hidden_size, filter_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.layer2 = nn.Linear(filter_size, hidden_size)\n",
    "\n",
    "        initialize_weight(self.layer1)\n",
    "        initialize_weight(self.layer2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout_rate, head_size=8):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.head_size = head_size\n",
    "\n",
    "        self.att_size = att_size = hidden_size // head_size\n",
    "        self.scale = att_size ** -0.5\n",
    "\n",
    "        self.linear_q = nn.Linear(hidden_size, head_size * att_size, bias=False)\n",
    "        self.linear_k = nn.Linear(hidden_size, head_size * att_size, bias=False)\n",
    "        self.linear_v = nn.Linear(hidden_size, head_size * att_size, bias=False)\n",
    "        initialize_weight(self.linear_q)\n",
    "        initialize_weight(self.linear_k)\n",
    "        initialize_weight(self.linear_v)\n",
    "\n",
    "        self.att_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.output_layer = nn.Linear(head_size * att_size, hidden_size,\n",
    "                                      bias=False)\n",
    "        initialize_weight(self.output_layer)\n",
    "\n",
    "    def forward(self, q, k, v, mask, cache=None):\n",
    "        orig_q_size = q.size()\n",
    "\n",
    "        d_k = self.att_size\n",
    "        d_v = self.att_size\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        # head_i = Attention(Q(W^Q)_i, K(W^K)_i, V(W^V)_i)\n",
    "        q = self.linear_q(q).view(batch_size, -1, self.head_size, d_k)\n",
    "        if cache is not None and 'encdec_k' in cache:\n",
    "            k, v = cache['encdec_k'], cache['encdec_v']\n",
    "        else:\n",
    "            k = self.linear_k(k).view(batch_size, -1, self.head_size, d_k)\n",
    "            v = self.linear_v(v).view(batch_size, -1, self.head_size, d_v)\n",
    "\n",
    "            if cache is not None:\n",
    "                cache['encdec_k'], cache['encdec_v'] = k, v\n",
    "\n",
    "        q = q.transpose(1, 2)                  # [b, h, q_len, d_k]\n",
    "        v = v.transpose(1, 2)                  # [b, h, v_len, d_v]\n",
    "        k = k.transpose(1, 2).transpose(2, 3)  # [b, h, d_k, k_len]\n",
    "\n",
    "        # Scaled Dot-Product Attention.\n",
    "        # Attention(Q, K, V) = softmax((QK^T)/sqrt(d_k))V\n",
    "        q.mul_(self.scale)\n",
    "        x = torch.matmul(q, k)  # [b, h, q_len, k_len]\n",
    "        x.masked_fill_(mask.unsqueeze(1), -1e9)\n",
    "        x = torch.softmax(x, dim=3)\n",
    "        x = self.att_dropout(x)\n",
    "        x = x.matmul(v)  # [b, h, q_len, attn]\n",
    "\n",
    "        x = x.transpose(1, 2).contiguous()  # [b, q_len, h, attn]\n",
    "        x = x.view(batch_size, -1, self.head_size * d_v)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        assert x.size() == orig_q_size\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.ffn_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.ffn = FeedForwardNetwork(hidden_size, filter_size, dropout_rate)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):  # pylint: disable=arguments-differ\n",
    "        y = self.self_attention_norm(x)\n",
    "        y = self.self_attention(y, y, y, mask)\n",
    "        y = self.self_attention_dropout(y)\n",
    "        x = x + y\n",
    "\n",
    "        y = self.ffn_norm(x)\n",
    "        y = self.ffn(y)\n",
    "        y = self.ffn_dropout(y)\n",
    "        x = x + y\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.self_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        self.self_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.enc_dec_attention_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.enc_dec_attention = MultiHeadAttention(hidden_size, dropout_rate)\n",
    "        self.enc_dec_attention_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.ffn_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "        self.ffn = FeedForwardNetwork(hidden_size, filter_size, dropout_rate)\n",
    "        self.ffn_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, enc_output, self_mask, i_mask, cache):\n",
    "        y = self.self_attention_norm(x)\n",
    "        y = self.self_attention(y, y, y, self_mask)\n",
    "        y = self.self_attention_dropout(y)\n",
    "        x = x + y\n",
    "\n",
    "        if enc_output is not None:\n",
    "            y = self.enc_dec_attention_norm(x)\n",
    "            y = self.enc_dec_attention(y, enc_output, enc_output, i_mask,\n",
    "                                       cache)\n",
    "            y = self.enc_dec_attention_dropout(y)\n",
    "            x = x + y\n",
    "\n",
    "        y = self.ffn_norm(x)\n",
    "        y = self.ffn(y)\n",
    "        y = self.ffn_dropout(y)\n",
    "        x = x + y\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate, n_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        encoders = [EncoderLayer(hidden_size, filter_size, dropout_rate)\n",
    "                    for _ in range(n_layers)]\n",
    "        self.layers = nn.ModuleList(encoders)\n",
    "\n",
    "        self.last_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "\n",
    "    def forward(self, inputs, mask):\n",
    "        encoder_output = inputs\n",
    "        for enc_layer in self.layers:\n",
    "            encoder_output = enc_layer(encoder_output, mask)\n",
    "        return self.last_norm(encoder_output)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, filter_size, dropout_rate, n_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        decoders = [DecoderLayer(hidden_size, filter_size, dropout_rate)\n",
    "                    for _ in range(n_layers)]\n",
    "        self.layers = nn.ModuleList(decoders)\n",
    "\n",
    "        self.last_norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
    "\n",
    "    def forward(self, targets, enc_output, i_mask, t_self_mask, cache):\n",
    "        decoder_output = targets\n",
    "        for i, dec_layer in enumerate(self.layers):\n",
    "            layer_cache = None\n",
    "            if cache is not None:\n",
    "                if i not in cache:\n",
    "                    cache[i] = {}\n",
    "                layer_cache = cache[i]\n",
    "            decoder_output = dec_layer(decoder_output, enc_output,\n",
    "                                       t_self_mask, i_mask, layer_cache)\n",
    "        return self.last_norm(decoder_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fdadba-025f-41a8-aab2-4e372cf8ac18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
